{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cad8cd4-2333-4f29-8102-c854b13ed191",
   "metadata": {},
   "source": [
    "Transformer From Scratch \n",
    "========================\n",
    "\n",
    "Reading along Dan Jurafsky and James H. Martin's [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) book, I decided to follow through Chapter 8 of their book to implement a Transformer using Pytorch. It is my goal to have a working transformer which I can use to train on the guitar dataset. I know some linear algebra, the book essentially gives the entire algorithm in terms of linear algebra, and pytorch provides a nice but still very informative abstractions for doing linear algebra. I had no reason not to pursue this project on top of whatever I proposed to do initially. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5d5ef-478a-418e-b06a-6b4ab9361952",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70625c-5684-4d78-b22c-26775a11202e",
   "metadata": {},
   "source": [
    "Attention Layer\n",
    "---------------\n",
    "\n",
    "At the heart of Transformer is the **attention layer**. It is a mechanism that allows words(tokens) to gain contextual meaning from their surrounding words(tokens). It can have multiple **\"heads\"**, where each \"head\" can be thought of as a specialist who asks particular set of questions given some data. For instance, one head could focus solely on grammar while another could instead focus on sentiments (even though that might not be exactly what occurs under the hood).\n",
    "\n",
    "Each head's job, then, is to ask the right kinds of *questions* to choose which of previous words it has seen matters the most to the current word. To do this, each head consists of three main components: **Query**, **Key**, and **Value** weight matrices. \n",
    "\n",
    "<!-- \n",
    "    essentially, what it is at the end of the day is weighted sum, but it's obviously lot more complicated than that\n",
    "    don't forget to write out the equations that I have referenced\n",
    "    maybe throw in some pictures\n",
    "    say something about how masking and softmax is used to determine what key's to focus on\n",
    "    also explain how results from different heads are consolidated at the end\n",
    "--!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b780d83-22ec-4a07-8671-302956801997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class AttentionLayer(nn.Module): \n",
    "    def __init__(self,\n",
    "                 N,         # context length; how many tokens to consider at any point in time\n",
    "                 model_dim, # d\n",
    "                 key_dim,   # d_k\n",
    "                 num_heads=1, \n",
    "                ):\n",
    "        super().__init__() \n",
    "\n",
    "        if num_heads < 1:\n",
    "            raise ValueError(\"num_heads cannot be less than 1!\")\n",
    "\n",
    "        if model_dim % num_heads != 0: \n",
    "            raise ValueError(\"model_dim is not divisible by num_heads!\")\n",
    "        \n",
    "        self.N = N\n",
    "        self.model_dim = model_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = model_dim//num_heads \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Query Weights (num_heads, model_dim, key_dim)\n",
    "        self.W_Q = nn.Parameter(torch.rand((num_heads, model_dim, key_dim)))\n",
    "        # Key Weights   (num_heads, model_dim, key_dim)\n",
    "        self.W_K = nn.Parameter(torch.rand((num_heads, model_dim, key_dim)))\n",
    "        # Value Weights (num_heads, model_dim, value_dim)\n",
    "        self.W_V = nn.Parameter(torch.rand((num_heads, model_dim, self.value_dim)))\n",
    "        # Output Weights\n",
    "        self.W_O = nn.Parameter(torch.rand((model_dim, model_dim)))\n",
    "\n",
    "        # Mask (for autoregressive model)\n",
    "        mask = torch.tensor([[0 if i>= j else -torch.inf for j in range(N)] for i in range(N)])\n",
    "        self.register_buffer(\"mask\", mask) # move mask to GPU\n",
    "        \n",
    "    def forward(self, X): # X has (N, model_dim) dimensions\n",
    "        seq_len = X.shape[0]\n",
    "        Q = X@self.W_Q # (num_heads, N, key_dim)\n",
    "        K = X@self.W_K # (num_heads, N, key_dim)\n",
    "        V = X@self.W_V # (num_heads, N, value_dim)\n",
    "\n",
    "        current_mask = self.mask[:seq_len, :seq_len] # when seq_len < N\n",
    "\n",
    "        attention = Q@(K.mT) / math.sqrt(self.key_dim) + current_mask # (num_heads, N (queries), N (keys)) \n",
    "        heads = nn.functional.softmax(attention, dim = -1)@V #(num_heads, N, value_dim)\n",
    "        cat_heads = torch.cat(heads.unbind(), dim=1) # (N, value_dim) each and concatenate the columns to form (N, model_dim)\n",
    "        A = cat_heads@self.W_O # (N, model_dim)\n",
    "\n",
    "        return A\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be67425b-4a19-431d-be3a-0cf9966aacae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7284, 1.4601, 1.0573, 2.4183],\n",
       "        [0.9968, 1.8307, 1.3983, 3.0009],\n",
       "        [0.9250, 1.8363, 1.3025, 3.0480]], device='cuda:0',\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((3,4)) # 3 words represented as dim (1, 4) tensors\n",
    "multihead_attention = AttentionLayer(N=3, model_dim=4, key_dim=2, num_heads=2)\n",
    "multihead_attention.to(\"cuda\")\n",
    "multihead_attention(X.to(\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e46ee8d-91a1-42b7-8d35-60d37716c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(in_features=model_dim, out_features=hidden_dim)\n",
    "        self.lin_2 = nn.Linear(in_features=hidden_dim, out_features=model_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, X):\n",
    "        return self.lin_2(self.relu(self.lin_1(X)))\n",
    "\n",
    "class LayerNorm(nn.Module): \n",
    "    def __init__(self, model_dim, epsilon=1e-6): \n",
    "        super().__init__()\n",
    "        # gamma and beta is to normalize features \n",
    "        self.gamma = nn.Parameter(torch.ones(model_dim)) # initialize gammas to ones because if initialized randomly to 0, it's dead signal\n",
    "        self.beta = nn.Parameter(torch.zeros(model_dim))\n",
    "        self.eps = epsilon\n",
    "    def forward(self, X): # (N, model_dim)\n",
    "        model_dim = X.shape[-1]\n",
    "        mean = torch.mean(X, dim=-1, keepdims=True)\n",
    "        std = torch.std(X, dim=-1, keepdims=True)\n",
    "        X_hat = (X - mean) / (std + self.eps) # add epsilon for numerical stability\n",
    "        layer_norm = self.gamma * X_hat + self.beta\n",
    "        return layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f47317f-3adc-4600-8013-7aad81f09f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8143,  0.7121, -0.1866, -1.3398],\n",
       "        [-0.4161,  1.1091, -1.1701,  0.4772],\n",
       "        [-0.1556,  0.6163,  0.8854, -1.3461]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = FeedForward(model_dim=4, hidden_dim=8)\n",
    "ff(X)\n",
    "norm = LayerNorm(model_dim=4)\n",
    "norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61771565-f4a4-404b-acfa-c9effe36067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module): \n",
    "    def __init__(self, \n",
    "                 N, \n",
    "                 model_dim, \n",
    "                 key_dim, \n",
    "                 hidden_dim, \n",
    "                 num_heads=1): \n",
    "        super().__init__() \n",
    "        self.N = N\n",
    "        self.model_dim = model_dim\n",
    "        self.key_dim = key_dim \n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.attention_layer = AttentionLayer(N=N, model_dim=model_dim, key_dim=key_dim, num_heads=num_heads)\n",
    "        self.ffn = FeedForward(model_dim=model_dim, hidden_dim=hidden_dim)\n",
    "        self.norm_1 = LayerNorm(model_dim=model_dim)\n",
    "        self.norm_2 = LayerNorm(model_dim=model_dim)\n",
    "\n",
    "    def forward(self,X): \n",
    "        T_1 = self.norm_1(X) \n",
    "        T_2 = self.attention_layer(T_1)\n",
    "        T_3 = T_2 + X\n",
    "        T_4 = self.norm_2(T_3)\n",
    "        T_5 = self.ffn(T_4)\n",
    "        H = T_5 + T_3 \n",
    "\n",
    "        return H\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "218580c8-ed94-466a-b480-1638781b64aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0590,  0.0180, -0.3674, -0.2382],\n",
       "        [ 0.6360,  0.5467, -0.3314,  0.2132],\n",
       "        [ 1.0297,  0.3499,  0.0983, -0.3943]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = TransformerBlock(N=3, model_dim=4, key_dim=2, hidden_dim=8, num_heads=2)\n",
    "block(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "380b1c0f-ebb3-4274-abef-d6eba5741559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPartial(nn.Module): \n",
    "    def __init__(self, \n",
    "                 N, \n",
    "                 model_dim, \n",
    "                 key_dim, \n",
    "                 hidden_dim, \n",
    "                 num_heads=1,\n",
    "                 num_stack=1\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_stack < 1: \n",
    "            raise ValueError(\"num_stack cannot be less than 1!\")\n",
    "\n",
    "        # missing language head, embedding/unembedding matricies\n",
    "        blocks = [TransformerBlock(N, model_dim, key_dim, hidden_dim, num_heads) for _ in range(num_stack)] \n",
    "        self.model = nn.Sequential(*blocks)\n",
    "    def forward(self,X): \n",
    "        return self.model(X) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "243f5b74-0960-42aa-9b8b-44c15c21a603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerPartial(\n",
       "  (model): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention_layer): AttentionLayer()\n",
       "      (ffn): FeedForward(\n",
       "        (lin_1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        (lin_2): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention_layer): AttentionLayer()\n",
       "      (ffn): FeedForward(\n",
       "        (lin_1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        (lin_2): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention_layer): AttentionLayer()\n",
       "      (ffn): FeedForward(\n",
       "        (lin_1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        (lin_2): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention_layer): AttentionLayer()\n",
       "      (ffn): FeedForward(\n",
       "        (lin_1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        (lin_2): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention_layer): AttentionLayer()\n",
       "      (ffn): FeedForward(\n",
       "        (lin_1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        (lin_2): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention_layer): AttentionLayer()\n",
       "      (ffn): FeedForward(\n",
       "        (lin_1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        (lin_2): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention_layer): AttentionLayer()\n",
       "      (ffn): FeedForward(\n",
       "        (lin_1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        (lin_2): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention_layer): AttentionLayer()\n",
       "      (ffn): FeedForward(\n",
       "        (lin_1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        (lin_2): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention_layer): AttentionLayer()\n",
       "      (ffn): FeedForward(\n",
       "        (lin_1): Linear(in_features=4, out_features=8, bias=True)\n",
       "        (lin_2): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerPartial(N=3, model_dim=4, key_dim=2, hidden_dim=8, num_heads=2, num_stack=9)\n",
    "model.state_dict()\n",
    "model.to('cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
