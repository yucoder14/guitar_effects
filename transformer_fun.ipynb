{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cad8cd4-2333-4f29-8102-c854b13ed191",
   "metadata": {},
   "source": [
    "Transformer From Scratch \n",
    "========================\n",
    "\n",
    "Reading along Dan Jurafsky and James H. Martin's [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) book, I decided to follow through Chapter 8 of their book to implement a Transformer using Pytorch. It is my goal to have a working transformer which I can use to train on the guitar dataset. I know some linear algebra, the book essentially gives the entire algorithm in terms of linear algebra, and pytorch provides a nice but still very informative abstractions for doing linear algebra. I had no reason not to pursue this project on top of whatever I proposed to do initially. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5d5ef-478a-418e-b06a-6b4ab9361952",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70625c-5684-4d78-b22c-26775a11202e",
   "metadata": {},
   "source": [
    "Attention Layer\n",
    "---------------\n",
    "\n",
    "At the heart of Transformer is the **attention layer**. It is a mechanism that allows words(tokens) to gain contextual meaning from their surrounding words(tokens). It can have multiple **\"heads\"**, where each \"head\" can be thought of as a specialist who asks particular set of questions given some data. For instance, one head could focus solely on grammar while another could instead focus on sentiments (even though that might not be exactly what occurs under the hood).\n",
    "\n",
    "Each head's job, then, is to ask the right kinds of *questions* to choose which of previous words it has seen matters the most to the current word. To do this, each head consists of three main components: **Query**, **Key**, and **Value** weight matrices. \n",
    "\n",
    "<!-- \n",
    "    essentially, what it is at the end of the day is weighted sum, but it's obviously lot more complicated than that\n",
    "    don't forget to write out the equations that I have referenced\n",
    "    maybe throw in some pictures\n",
    "    say something about how masking and softmax is used to determine what key's to focus on\n",
    "    also explain how results from different heads are consolidated at the end\n",
    "--!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b780d83-22ec-4a07-8671-302956801997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class AttentionLayer(nn.Module): \n",
    "    def __init__(self,\n",
    "                 N,         # context length; how many tokens to consider at any point in time\n",
    "                 model_dim, # d\n",
    "                 key_dim,   # d_k\n",
    "                 num_heads=1, \n",
    "                ):\n",
    "        super().__init__() \n",
    "\n",
    "        if num_heads < 1:\n",
    "            raise ValueError(\"num_heads cannot be less than 1!\")\n",
    "\n",
    "        if model_dim % num_heads != 0: \n",
    "            raise ValueError(\"model_dim is not divisible by num_heads!\")\n",
    "        \n",
    "        self.N = N\n",
    "        self.model_dim = model_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = model_dim//num_heads \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Query Weights (num_heads, model_dim, key_dim)\n",
    "        self.W_Q = nn.Parameter(torch.rand((num_heads, model_dim, key_dim)))\n",
    "        # Key Weights   (num_heads, model_dim, key_dim)\n",
    "        self.W_K = nn.Parameter(torch.rand((num_heads, model_dim, key_dim)))\n",
    "        # Value Weights (num_heads, model_dim, value_dim)\n",
    "        self.W_V = nn.Parameter(torch.rand((num_heads, model_dim, self.value_dim)))\n",
    "        # Output Weights\n",
    "        self.W_O = nn.Parameter(torch.rand((model_dim, model_dim)))\n",
    "\n",
    "        # Mask (for autoregressive model)\n",
    "        mask = torch.tensor([[0 if i>= j else -torch.inf for j in range(N)] for i in range(N)])\n",
    "        self.register_buffer(\"mask\", mask) # move mask to GPU\n",
    "        \n",
    "    def forward(self, X): # X has (N, model_dim) dimensions\n",
    "        seq_len = X.shape[0]\n",
    "        Q = X@self.W_Q # (num_heads, N, key_dim)\n",
    "        K = X@self.W_K # (num_heads, N, key_dim)\n",
    "        V = X@self.W_V # (num_heads, N, value_dim)\n",
    "\n",
    "        current_mask = self.mask[:seq_len, :seq_len] # when seq_len < N\n",
    "\n",
    "        attention = Q@(K.mT) / math.sqrt(self.key_dim) + current_mask # (num_heads, N (queries), N (keys)) \n",
    "        heads = nn.functional.softmax(attention, dim = -1)@V #(num_heads, N, value_dim)\n",
    "        cat_heads = torch.cat(heads.unbind(), dim=1) # (N, value_dim) each and concatenate the columns to form (N, model_dim)\n",
    "        A = cat_heads@self.W_O # (N, model_dim)\n",
    "\n",
    "        return A\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be67425b-4a19-431d-be3a-0cf9966aacae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7284, 1.4601, 1.0573, 2.4183],\n",
       "        [0.9968, 1.8307, 1.3983, 3.0009],\n",
       "        [0.9250, 1.8363, 1.3025, 3.0480]], device='cuda:0',\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((3,4)) # 3 words represented as dim (1, 4) tensors\n",
    "multihead_attention = AttentionLayer(N=3, model_dim=4, key_dim=2, num_heads=2)\n",
    "multihead_attention.to(\"cuda\")\n",
    "multihead_attention(X.to(\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e46ee8d-91a1-42b7-8d35-60d37716c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(in_features=model_dim, out_features=hidden_dim)\n",
    "        self.lin_2 = nn.Linear(in_features=hidden_dim, out_features=model_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, X):\n",
    "        return self.lin_2(self.relu(self.lin_1(X)))\n",
    "\n",
    "class LayerNorm(nn.Module): \n",
    "    def __init__(self, model_dim, epsilon=1e-6): \n",
    "        super().__init__()\n",
    "        # gamma and beta is to normalize features \n",
    "        self.gamma = nn.Parameter(torch.ones(model_dim)) # initialize gammas to ones because if initialized randomly to 0, it's dead signal\n",
    "        self.beta = nn.Parameter(torch.zeros(model_dim))\n",
    "        self.eps = epsilon\n",
    "    def forward(self, X): # (N, model_dim)\n",
    "        model_dim = X.shape[-1]\n",
    "        mean = torch.mean(X, dim=-1, keepdims=True)\n",
    "        std = torch.std(X, dim=-1, keepdims=True)\n",
    "        X_hat = (X - mean) / (std + self.eps) # add epsilon for numerical stability\n",
    "        layer_norm = self.gamma * X_hat + self.beta\n",
    "        return layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f47317f-3adc-4600-8013-7aad81f09f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8143,  0.7121, -0.1866, -1.3398],\n",
       "        [-0.4161,  1.1091, -1.1701,  0.4772],\n",
       "        [-0.1556,  0.6163,  0.8854, -1.3461]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = FeedForward(model_dim=4, hidden_dim=8)\n",
    "ff(X)\n",
    "norm = LayerNorm(model_dim=4)\n",
    "norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61771565-f4a4-404b-acfa-c9effe36067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module): \n",
    "    def __init__(self, \n",
    "                 N, \n",
    "                 model_dim, \n",
    "                 key_dim, \n",
    "                 hidden_dim, \n",
    "                 num_heads=1): \n",
    "        super().__init__() \n",
    "        self.N = N\n",
    "        self.model_dim = model_dim\n",
    "        self.key_dim = key_dim \n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.attention_layer = AttentionLayer(N=N, model_dim=model_dim, key_dim=key_dim, num_heads=num_heads)\n",
    "        self.ffn = FeedForward(model_dim=model_dim, hidden_dim=hidden_dim)\n",
    "        self.norm_1 = LayerNorm(model_dim=model_dim)\n",
    "        self.norm_2 = LayerNorm(model_dim=model_dim)\n",
    "\n",
    "    def forward(self,X): \n",
    "        T_1 = self.norm_1(X) \n",
    "        T_2 = self.attention_layer(T_1)\n",
    "        T_3 = T_2 + X\n",
    "        T_4 = self.norm_2(T_3)\n",
    "        T_5 = self.ffn(T_4)\n",
    "        H = T_5 + T_3 \n",
    "\n",
    "        return H\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "218580c8-ed94-466a-b480-1638781b64aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0590,  0.0180, -0.3674, -0.2382],\n",
       "        [ 0.6360,  0.5467, -0.3314,  0.2132],\n",
       "        [ 1.0297,  0.3499,  0.0983, -0.3943]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = TransformerBlock(N=3, model_dim=4, key_dim=2, hidden_dim=8, num_heads=2)\n",
    "block(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "380b1c0f-ebb3-4274-abef-d6eba5741559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module): \n",
    "    def __init__(self, \n",
    "                 N, \n",
    "                 model_dim, \n",
    "                 key_dim, \n",
    "                 hidden_dim, \n",
    "                 num_heads=1,\n",
    "                 num_stack=1\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_stack < 1: \n",
    "            raise ValueError(\"num_stack cannot be less than 1!\")\n",
    "\n",
    "        # missing language head, embedding/unembedding matricies\n",
    "        blocks = [TransformerBlock(N, model_dim, key_dim, hidden_dim, num_heads) for _ in range(num_stack)] \n",
    "        self.model = nn.Sequential(*blocks)\n",
    "    def forward(self,X): \n",
    "        return self.model(X) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "243f5b74-0960-42aa-9b8b-44c15c21a603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.0.attention_layer.W_Q',\n",
       "              tensor([[[0.5050, 0.2384],\n",
       "                       [0.7371, 0.0694],\n",
       "                       [0.2770, 0.8450],\n",
       "                       [0.1893, 0.7880]],\n",
       "              \n",
       "                      [[0.0665, 0.0112],\n",
       "                       [0.9444, 0.6999],\n",
       "                       [0.4469, 0.6458],\n",
       "                       [0.7735, 0.2013]]])),\n",
       "             ('model.0.attention_layer.W_K',\n",
       "              tensor([[[0.2654, 0.5657],\n",
       "                       [0.2708, 0.6153],\n",
       "                       [0.1084, 0.9999],\n",
       "                       [0.2192, 0.9200]],\n",
       "              \n",
       "                      [[0.6538, 0.9476],\n",
       "                       [0.9976, 0.9429],\n",
       "                       [0.2572, 0.9861],\n",
       "                       [0.7417, 0.9392]]])),\n",
       "             ('model.0.attention_layer.W_V',\n",
       "              tensor([[[0.8646, 0.3119],\n",
       "                       [0.4881, 0.1162],\n",
       "                       [0.6396, 0.5684],\n",
       "                       [0.2829, 0.1063]],\n",
       "              \n",
       "                      [[0.2424, 0.1769],\n",
       "                       [0.7624, 0.3885],\n",
       "                       [0.4888, 0.0456],\n",
       "                       [0.8974, 0.6769]]])),\n",
       "             ('model.0.attention_layer.W_O',\n",
       "              tensor([[0.3233, 0.6391, 0.0373, 0.1207],\n",
       "                      [0.0739, 0.3835, 0.7601, 0.0926],\n",
       "                      [0.5682, 0.2273, 0.4823, 0.7880],\n",
       "                      [0.2783, 0.4117, 0.3168, 0.3748]])),\n",
       "             ('model.0.attention_layer.mask',\n",
       "              tensor([[0., -inf, -inf],\n",
       "                      [0., 0., -inf],\n",
       "                      [0., 0., 0.]])),\n",
       "             ('model.0.ffn.lin_1.weight',\n",
       "              tensor([[ 0.0146,  0.3099, -0.4474,  0.4988],\n",
       "                      [-0.2685, -0.0131,  0.3262, -0.3005],\n",
       "                      [ 0.0783,  0.2669, -0.2758, -0.0388],\n",
       "                      [ 0.2762,  0.2498,  0.1579,  0.3153],\n",
       "                      [ 0.3976, -0.2302,  0.4733,  0.3370],\n",
       "                      [ 0.1720, -0.4190,  0.1295,  0.4059],\n",
       "                      [ 0.4941, -0.1768, -0.0626,  0.4948],\n",
       "                      [ 0.1639, -0.4279,  0.2309,  0.0074]])),\n",
       "             ('model.0.ffn.lin_1.bias',\n",
       "              tensor([-0.3371,  0.0096, -0.3362,  0.2507, -0.2585,  0.2035,  0.1537,  0.3185])),\n",
       "             ('model.0.ffn.lin_2.weight',\n",
       "              tensor([[-0.1283, -0.0796,  0.2551, -0.3330,  0.0800, -0.3185, -0.2638, -0.3050],\n",
       "                      [-0.2786, -0.2335, -0.1821,  0.2027,  0.0941, -0.0463, -0.1928, -0.0981],\n",
       "                      [-0.1183, -0.2586,  0.1226, -0.3413, -0.0057,  0.0219,  0.1234,  0.0862],\n",
       "                      [-0.0065,  0.2461, -0.0627,  0.1944, -0.3060,  0.2571, -0.3075, -0.3197]])),\n",
       "             ('model.0.ffn.lin_2.bias',\n",
       "              tensor([ 0.1091, -0.1547,  0.0880,  0.0805])),\n",
       "             ('model.0.norm.gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('model.0.norm.beta', tensor([0., 0., 0., 0.])),\n",
       "             ('model.1.attention_layer.W_Q',\n",
       "              tensor([[[0.3282, 0.0311],\n",
       "                       [0.5714, 0.2954],\n",
       "                       [0.0726, 0.3730],\n",
       "                       [0.1304, 0.9395]],\n",
       "              \n",
       "                      [[0.3482, 0.3292],\n",
       "                       [0.0377, 0.8854],\n",
       "                       [0.5338, 0.5777],\n",
       "                       [0.4370, 0.7748]]])),\n",
       "             ('model.1.attention_layer.W_K',\n",
       "              tensor([[[0.7939, 0.7517],\n",
       "                       [0.2221, 0.3062],\n",
       "                       [0.6285, 0.5215],\n",
       "                       [0.9888, 0.7647]],\n",
       "              \n",
       "                      [[0.9908, 0.9537],\n",
       "                       [0.3291, 0.6705],\n",
       "                       [0.8409, 0.3947],\n",
       "                       [0.5834, 0.0573]]])),\n",
       "             ('model.1.attention_layer.W_V',\n",
       "              tensor([[[0.9650, 0.7815],\n",
       "                       [0.3734, 0.1496],\n",
       "                       [0.4416, 0.2927],\n",
       "                       [0.5015, 0.3194]],\n",
       "              \n",
       "                      [[0.9948, 0.8441],\n",
       "                       [0.0078, 0.5640],\n",
       "                       [0.6223, 0.6321],\n",
       "                       [0.5849, 0.5367]]])),\n",
       "             ('model.1.attention_layer.W_O',\n",
       "              tensor([[0.2874, 0.3012, 0.1504, 0.0630],\n",
       "                      [0.8983, 0.1034, 0.9215, 0.8110],\n",
       "                      [0.3364, 0.2240, 0.9369, 0.4894],\n",
       "                      [0.4091, 0.8666, 0.6647, 0.7413]])),\n",
       "             ('model.1.attention_layer.mask',\n",
       "              tensor([[0., -inf, -inf],\n",
       "                      [0., 0., -inf],\n",
       "                      [0., 0., 0.]])),\n",
       "             ('model.1.ffn.lin_1.weight',\n",
       "              tensor([[-0.4900,  0.0466, -0.0649, -0.2051],\n",
       "                      [ 0.3189, -0.1648,  0.1110,  0.3683],\n",
       "                      [ 0.2026, -0.0652,  0.0100, -0.1636],\n",
       "                      [-0.4528, -0.4500,  0.3988,  0.1428],\n",
       "                      [ 0.4769, -0.4241,  0.0958, -0.1010],\n",
       "                      [ 0.4058,  0.2372, -0.3322,  0.2448],\n",
       "                      [ 0.2434,  0.3417,  0.1393,  0.3381],\n",
       "                      [ 0.1180, -0.0217, -0.1706,  0.0778]])),\n",
       "             ('model.1.ffn.lin_1.bias',\n",
       "              tensor([ 0.0094,  0.0503, -0.4766,  0.0663,  0.2098,  0.1004,  0.0206, -0.4269])),\n",
       "             ('model.1.ffn.lin_2.weight',\n",
       "              tensor([[ 0.0902, -0.2091, -0.2688,  0.0625, -0.3282,  0.3257,  0.3132, -0.1898],\n",
       "                      [ 0.2378, -0.3279,  0.2905, -0.1105, -0.1695, -0.2135, -0.0579, -0.1993],\n",
       "                      [ 0.1745, -0.2569,  0.0777,  0.2873, -0.2812,  0.2251, -0.3521,  0.0518],\n",
       "                      [-0.2171, -0.1537, -0.1089, -0.2994, -0.2653,  0.2926,  0.2118,  0.0229]])),\n",
       "             ('model.1.ffn.lin_2.bias',\n",
       "              tensor([-0.0374, -0.1713, -0.3187,  0.1403])),\n",
       "             ('model.1.norm.gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('model.1.norm.beta', tensor([0., 0., 0., 0.])),\n",
       "             ('model.2.attention_layer.W_Q',\n",
       "              tensor([[[0.3895, 0.1420],\n",
       "                       [0.8647, 0.1159],\n",
       "                       [0.0471, 0.4170],\n",
       "                       [0.8734, 0.5640]],\n",
       "              \n",
       "                      [[0.0985, 0.8764],\n",
       "                       [0.3320, 0.4472],\n",
       "                       [0.4366, 0.7371],\n",
       "                       [0.2117, 0.7102]]])),\n",
       "             ('model.2.attention_layer.W_K',\n",
       "              tensor([[[0.8902, 0.6949],\n",
       "                       [0.9699, 0.6202],\n",
       "                       [0.3349, 0.2225],\n",
       "                       [0.0618, 0.1898]],\n",
       "              \n",
       "                      [[0.6187, 0.0072],\n",
       "                       [0.8037, 0.3912],\n",
       "                       [0.3933, 0.4627],\n",
       "                       [0.0599, 0.4445]]])),\n",
       "             ('model.2.attention_layer.W_V',\n",
       "              tensor([[[0.1568, 0.5215],\n",
       "                       [0.4632, 0.2095],\n",
       "                       [0.2195, 0.5469],\n",
       "                       [0.5141, 0.9683]],\n",
       "              \n",
       "                      [[0.0616, 0.7907],\n",
       "                       [0.7699, 0.8861],\n",
       "                       [0.0208, 0.0446],\n",
       "                       [0.4118, 0.0473]]])),\n",
       "             ('model.2.attention_layer.W_O',\n",
       "              tensor([[0.5913, 0.5500, 0.0627, 0.1146],\n",
       "                      [0.6430, 0.4395, 0.2579, 0.0328],\n",
       "                      [0.2580, 0.5304, 0.0881, 0.6752],\n",
       "                      [0.4763, 0.3642, 0.7114, 0.7197]])),\n",
       "             ('model.2.attention_layer.mask',\n",
       "              tensor([[0., -inf, -inf],\n",
       "                      [0., 0., -inf],\n",
       "                      [0., 0., 0.]])),\n",
       "             ('model.2.ffn.lin_1.weight',\n",
       "              tensor([[ 0.0878, -0.0255, -0.4910, -0.4157],\n",
       "                      [-0.1150,  0.1022, -0.4051, -0.1985],\n",
       "                      [ 0.2064,  0.0931, -0.4663, -0.0517],\n",
       "                      [-0.2279, -0.0311,  0.1550, -0.0943],\n",
       "                      [-0.0700, -0.1450,  0.0322,  0.4518],\n",
       "                      [ 0.2202, -0.3985,  0.0861, -0.3965],\n",
       "                      [-0.4463, -0.0400, -0.4027, -0.1109],\n",
       "                      [ 0.3065, -0.0009, -0.4872,  0.3782]])),\n",
       "             ('model.2.ffn.lin_1.bias',\n",
       "              tensor([-0.3707,  0.3997, -0.3869, -0.4605, -0.1651,  0.4006, -0.2898, -0.4467])),\n",
       "             ('model.2.ffn.lin_2.weight',\n",
       "              tensor([[ 0.0534,  0.2519,  0.0053, -0.1562, -0.2069, -0.3373,  0.0108, -0.0555],\n",
       "                      [-0.0111,  0.1457, -0.3068,  0.0386, -0.3205, -0.2322, -0.0884,  0.0779],\n",
       "                      [ 0.3185, -0.2070, -0.2005, -0.1976,  0.2507, -0.0107, -0.3014,  0.1830],\n",
       "                      [ 0.2388, -0.1091,  0.1157,  0.1226, -0.0822,  0.0113, -0.1408,  0.2835]])),\n",
       "             ('model.2.ffn.lin_2.bias',\n",
       "              tensor([-0.1150,  0.0662, -0.0173,  0.0058])),\n",
       "             ('model.2.norm.gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('model.2.norm.beta', tensor([0., 0., 0., 0.])),\n",
       "             ('model.3.attention_layer.W_Q',\n",
       "              tensor([[[0.6914, 0.8332],\n",
       "                       [0.3075, 0.3593],\n",
       "                       [0.1992, 0.0016],\n",
       "                       [0.4041, 0.4099]],\n",
       "              \n",
       "                      [[0.7150, 0.7953],\n",
       "                       [0.7813, 0.3500],\n",
       "                       [0.5937, 0.2224],\n",
       "                       [0.5707, 0.5265]]])),\n",
       "             ('model.3.attention_layer.W_K',\n",
       "              tensor([[[0.9085, 0.5195],\n",
       "                       [0.9108, 0.6442],\n",
       "                       [0.6023, 0.1923],\n",
       "                       [0.9052, 0.2064]],\n",
       "              \n",
       "                      [[0.7436, 0.1279],\n",
       "                       [0.1962, 0.7452],\n",
       "                       [0.0553, 0.4451],\n",
       "                       [0.2749, 0.0667]]])),\n",
       "             ('model.3.attention_layer.W_V',\n",
       "              tensor([[[0.9878, 0.3628],\n",
       "                       [0.9821, 0.4346],\n",
       "                       [0.8306, 0.0066],\n",
       "                       [0.1972, 0.1062]],\n",
       "              \n",
       "                      [[0.7011, 0.5660],\n",
       "                       [0.0198, 0.4636],\n",
       "                       [0.4834, 0.9773],\n",
       "                       [0.6438, 0.5910]]])),\n",
       "             ('model.3.attention_layer.W_O',\n",
       "              tensor([[0.4461, 0.6774, 0.6020, 0.6372],\n",
       "                      [0.3204, 0.2492, 0.1340, 0.2638],\n",
       "                      [0.4612, 0.4770, 0.3876, 0.1437],\n",
       "                      [0.8409, 0.7424, 0.4854, 0.7854]])),\n",
       "             ('model.3.attention_layer.mask',\n",
       "              tensor([[0., -inf, -inf],\n",
       "                      [0., 0., -inf],\n",
       "                      [0., 0., 0.]])),\n",
       "             ('model.3.ffn.lin_1.weight',\n",
       "              tensor([[ 0.3761, -0.1976, -0.1100, -0.0458],\n",
       "                      [ 0.1187,  0.3912, -0.1145,  0.2963],\n",
       "                      [-0.2766, -0.0133, -0.2451,  0.3016],\n",
       "                      [-0.0299, -0.0680, -0.4308,  0.0315],\n",
       "                      [ 0.1869, -0.1915,  0.2806,  0.1681],\n",
       "                      [ 0.0245,  0.4915, -0.0917,  0.2338],\n",
       "                      [ 0.3599, -0.3083, -0.0325, -0.3951],\n",
       "                      [ 0.0928, -0.1739, -0.4657, -0.3990]])),\n",
       "             ('model.3.ffn.lin_1.bias',\n",
       "              tensor([ 0.0572, -0.1106,  0.4464,  0.3252,  0.1892,  0.2569,  0.4562,  0.3091])),\n",
       "             ('model.3.ffn.lin_2.weight',\n",
       "              tensor([[ 0.1906,  0.2233, -0.3270, -0.3171, -0.3494,  0.1270,  0.2082,  0.1515],\n",
       "                      [-0.3061,  0.1131,  0.3138, -0.3124,  0.0153,  0.2619, -0.2320,  0.2397],\n",
       "                      [ 0.3457,  0.0514, -0.0069,  0.1275, -0.0256,  0.2838,  0.2939,  0.0186],\n",
       "                      [-0.2142,  0.0817,  0.1955,  0.0192, -0.0318,  0.2045,  0.1085, -0.0473]])),\n",
       "             ('model.3.ffn.lin_2.bias',\n",
       "              tensor([-0.3367,  0.3500, -0.3496, -0.3010])),\n",
       "             ('model.3.norm.gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('model.3.norm.beta', tensor([0., 0., 0., 0.])),\n",
       "             ('model.4.attention_layer.W_Q',\n",
       "              tensor([[[0.5311, 0.2084],\n",
       "                       [0.0407, 0.6720],\n",
       "                       [0.0664, 0.6414],\n",
       "                       [0.9762, 0.1338]],\n",
       "              \n",
       "                      [[0.8208, 0.3980],\n",
       "                       [0.7886, 0.8606],\n",
       "                       [0.5371, 0.9380],\n",
       "                       [0.5540, 0.0871]]])),\n",
       "             ('model.4.attention_layer.W_K',\n",
       "              tensor([[[0.1685, 0.8648],\n",
       "                       [0.7524, 0.7351],\n",
       "                       [0.2858, 0.8535],\n",
       "                       [0.6902, 0.2624]],\n",
       "              \n",
       "                      [[0.8047, 0.9072],\n",
       "                       [0.6204, 0.3749],\n",
       "                       [0.8458, 0.9329],\n",
       "                       [0.4047, 0.0449]]])),\n",
       "             ('model.4.attention_layer.W_V',\n",
       "              tensor([[[0.7522, 0.4327],\n",
       "                       [0.7343, 0.0968],\n",
       "                       [0.8066, 0.2340],\n",
       "                       [0.2290, 0.0095]],\n",
       "              \n",
       "                      [[0.4696, 0.0574],\n",
       "                       [0.0665, 0.6579],\n",
       "                       [0.8694, 0.3369],\n",
       "                       [0.2538, 0.5393]]])),\n",
       "             ('model.4.attention_layer.W_O',\n",
       "              tensor([[0.7960, 0.6895, 0.3194, 0.7825],\n",
       "                      [0.5983, 0.4203, 0.8471, 0.2191],\n",
       "                      [0.5114, 0.0059, 0.5301, 0.0867],\n",
       "                      [0.1438, 0.2371, 0.2644, 0.4046]])),\n",
       "             ('model.4.attention_layer.mask',\n",
       "              tensor([[0., -inf, -inf],\n",
       "                      [0., 0., -inf],\n",
       "                      [0., 0., 0.]])),\n",
       "             ('model.4.ffn.lin_1.weight',\n",
       "              tensor([[-0.2100, -0.2917, -0.4904, -0.4211],\n",
       "                      [-0.3517, -0.1622, -0.0056,  0.0141],\n",
       "                      [-0.3575,  0.3484,  0.2733,  0.0118],\n",
       "                      [ 0.3420, -0.2982,  0.0912, -0.0593],\n",
       "                      [-0.1420,  0.3356, -0.0769,  0.4424],\n",
       "                      [ 0.2409,  0.3707,  0.3627, -0.2089],\n",
       "                      [ 0.2071,  0.1484, -0.2657, -0.4693],\n",
       "                      [-0.1049,  0.1344,  0.0415, -0.2137]])),\n",
       "             ('model.4.ffn.lin_1.bias',\n",
       "              tensor([ 0.4170,  0.3907,  0.4928, -0.3725, -0.0295,  0.2470, -0.0017, -0.1452])),\n",
       "             ('model.4.ffn.lin_2.weight',\n",
       "              tensor([[-0.0599, -0.1577, -0.2337,  0.3511, -0.3461, -0.0549,  0.2270,  0.0290],\n",
       "                      [ 0.0675,  0.1161,  0.3355,  0.1572, -0.0871,  0.1417,  0.0170,  0.3086],\n",
       "                      [ 0.0167, -0.2031,  0.2831, -0.2658,  0.1279,  0.2904,  0.1386,  0.1267],\n",
       "                      [-0.0162, -0.3337,  0.3416, -0.0694,  0.3506, -0.0798, -0.1540,  0.0053]])),\n",
       "             ('model.4.ffn.lin_2.bias',\n",
       "              tensor([-0.0982, -0.0463,  0.0508,  0.0117])),\n",
       "             ('model.4.norm.gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('model.4.norm.beta', tensor([0., 0., 0., 0.])),\n",
       "             ('model.5.attention_layer.W_Q',\n",
       "              tensor([[[0.3179, 0.9981],\n",
       "                       [0.3030, 0.9522],\n",
       "                       [0.1844, 0.6777],\n",
       "                       [0.1012, 0.8349]],\n",
       "              \n",
       "                      [[0.6839, 0.5037],\n",
       "                       [0.6109, 0.7413],\n",
       "                       [0.4305, 0.6882],\n",
       "                       [0.3853, 0.8940]]])),\n",
       "             ('model.5.attention_layer.W_K',\n",
       "              tensor([[[0.0134, 0.8256],\n",
       "                       [0.9934, 0.7560],\n",
       "                       [0.7048, 0.9258],\n",
       "                       [0.0346, 0.4738]],\n",
       "              \n",
       "                      [[0.1223, 0.1397],\n",
       "                       [0.1408, 0.7369],\n",
       "                       [0.0980, 0.4844],\n",
       "                       [0.5918, 0.8842]]])),\n",
       "             ('model.5.attention_layer.W_V',\n",
       "              tensor([[[0.7295, 0.1754],\n",
       "                       [0.7476, 0.0162],\n",
       "                       [0.3691, 0.0288],\n",
       "                       [0.4652, 0.5694]],\n",
       "              \n",
       "                      [[0.2615, 0.8154],\n",
       "                       [0.4635, 0.8816],\n",
       "                       [0.7009, 0.0229],\n",
       "                       [0.0862, 0.0150]]])),\n",
       "             ('model.5.attention_layer.W_O',\n",
       "              tensor([[0.3765, 0.5749, 0.9343, 0.0867],\n",
       "                      [0.1662, 0.3388, 0.1315, 0.9609],\n",
       "                      [0.5692, 0.1981, 0.0956, 0.8322],\n",
       "                      [0.8567, 0.4090, 0.1162, 0.0132]])),\n",
       "             ('model.5.attention_layer.mask',\n",
       "              tensor([[0., -inf, -inf],\n",
       "                      [0., 0., -inf],\n",
       "                      [0., 0., 0.]])),\n",
       "             ('model.5.ffn.lin_1.weight',\n",
       "              tensor([[-0.3580,  0.4737, -0.3914, -0.4221],\n",
       "                      [-0.4783, -0.4978, -0.4923,  0.1987],\n",
       "                      [-0.2972,  0.0112,  0.1611,  0.4816],\n",
       "                      [-0.3168, -0.3800,  0.1531,  0.3823],\n",
       "                      [ 0.2295, -0.2380, -0.1972, -0.4006],\n",
       "                      [ 0.3188,  0.1351,  0.1433,  0.3081],\n",
       "                      [-0.0932, -0.0426,  0.4905,  0.3108],\n",
       "                      [ 0.0607,  0.2332,  0.1554,  0.4714]])),\n",
       "             ('model.5.ffn.lin_1.bias',\n",
       "              tensor([-0.3457, -0.3646, -0.2241,  0.3629,  0.4241, -0.2141, -0.4821,  0.0519])),\n",
       "             ('model.5.ffn.lin_2.weight',\n",
       "              tensor([[-0.1471,  0.1359, -0.0676,  0.0654,  0.3356,  0.3409, -0.1531,  0.0453],\n",
       "                      [ 0.0095, -0.1783, -0.2883, -0.1702, -0.1350, -0.3308, -0.1762,  0.1473],\n",
       "                      [-0.0730, -0.1821,  0.2156,  0.2940,  0.2549, -0.0352, -0.0121, -0.1873],\n",
       "                      [-0.2785,  0.0141,  0.1803, -0.1286, -0.0810,  0.0196,  0.2603, -0.2264]])),\n",
       "             ('model.5.ffn.lin_2.bias',\n",
       "              tensor([-0.1353, -0.3353, -0.2809, -0.1537])),\n",
       "             ('model.5.norm.gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('model.5.norm.beta', tensor([0., 0., 0., 0.])),\n",
       "             ('model.6.attention_layer.W_Q',\n",
       "              tensor([[[0.6519, 0.3638],\n",
       "                       [0.6086, 0.0873],\n",
       "                       [0.4390, 0.0696],\n",
       "                       [0.3486, 0.6673]],\n",
       "              \n",
       "                      [[0.6181, 0.8193],\n",
       "                       [0.5808, 0.9461],\n",
       "                       [0.6695, 0.2728],\n",
       "                       [0.5400, 0.9397]]])),\n",
       "             ('model.6.attention_layer.W_K',\n",
       "              tensor([[[0.9639, 0.7611],\n",
       "                       [0.3957, 0.9989],\n",
       "                       [0.5680, 0.0358],\n",
       "                       [0.6422, 0.8520]],\n",
       "              \n",
       "                      [[0.2411, 0.8787],\n",
       "                       [0.8656, 0.1662],\n",
       "                       [0.6134, 0.2894],\n",
       "                       [0.5687, 0.8228]]])),\n",
       "             ('model.6.attention_layer.W_V',\n",
       "              tensor([[[0.5750, 0.0113],\n",
       "                       [0.7342, 0.7942],\n",
       "                       [0.8326, 0.6260],\n",
       "                       [0.0602, 0.2685]],\n",
       "              \n",
       "                      [[0.9015, 0.3003],\n",
       "                       [0.5708, 0.6748],\n",
       "                       [0.3502, 0.6241],\n",
       "                       [0.3593, 0.0432]]])),\n",
       "             ('model.6.attention_layer.W_O',\n",
       "              tensor([[0.2689, 0.5561, 0.1525, 0.6727],\n",
       "                      [0.0272, 0.0058, 0.0890, 0.6429],\n",
       "                      [0.5957, 0.9873, 0.3983, 0.7323],\n",
       "                      [0.3683, 0.3628, 0.8802, 0.5563]])),\n",
       "             ('model.6.attention_layer.mask',\n",
       "              tensor([[0., -inf, -inf],\n",
       "                      [0., 0., -inf],\n",
       "                      [0., 0., 0.]])),\n",
       "             ('model.6.ffn.lin_1.weight',\n",
       "              tensor([[-0.2272, -0.2219,  0.1410, -0.3981],\n",
       "                      [ 0.2632,  0.3865, -0.0021,  0.2549],\n",
       "                      [-0.1828, -0.4608, -0.1108,  0.3839],\n",
       "                      [ 0.0350,  0.3102,  0.0575, -0.1014],\n",
       "                      [ 0.3110,  0.4035, -0.1905, -0.1882],\n",
       "                      [-0.1226, -0.2465, -0.3260,  0.0081],\n",
       "                      [-0.4447, -0.1127,  0.3676, -0.0556],\n",
       "                      [-0.3710, -0.4267, -0.0726,  0.0459]])),\n",
       "             ('model.6.ffn.lin_1.bias',\n",
       "              tensor([-0.3955,  0.0518, -0.1865,  0.2673,  0.3814, -0.4606,  0.4914,  0.1413])),\n",
       "             ('model.6.ffn.lin_2.weight',\n",
       "              tensor([[-0.1425, -0.1324,  0.2092, -0.0421, -0.2586,  0.2898, -0.3285,  0.3432],\n",
       "                      [-0.0206, -0.0246, -0.2483, -0.1068,  0.1443, -0.1411, -0.0902, -0.2805],\n",
       "                      [-0.0737,  0.0513,  0.2120,  0.0710, -0.0970, -0.0876, -0.1222,  0.1594],\n",
       "                      [-0.3428, -0.2286, -0.0912, -0.2390, -0.1742,  0.0846, -0.1633, -0.3264]])),\n",
       "             ('model.6.ffn.lin_2.bias',\n",
       "              tensor([-0.0326, -0.1133,  0.0265,  0.2119])),\n",
       "             ('model.6.norm.gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('model.6.norm.beta', tensor([0., 0., 0., 0.])),\n",
       "             ('model.7.attention_layer.W_Q',\n",
       "              tensor([[[0.0066, 0.6222],\n",
       "                       [0.5376, 0.7580],\n",
       "                       [0.7292, 0.3811],\n",
       "                       [0.1412, 0.3309]],\n",
       "              \n",
       "                      [[0.8279, 0.3934],\n",
       "                       [0.7396, 0.5270],\n",
       "                       [0.2142, 0.1956],\n",
       "                       [0.8010, 0.2822]]])),\n",
       "             ('model.7.attention_layer.W_K',\n",
       "              tensor([[[0.6165, 0.9379],\n",
       "                       [0.9591, 0.9252],\n",
       "                       [0.2431, 0.5918],\n",
       "                       [0.4010, 0.4887]],\n",
       "              \n",
       "                      [[0.5905, 0.3772],\n",
       "                       [0.8063, 0.5678],\n",
       "                       [0.9838, 0.5541],\n",
       "                       [0.0437, 0.3746]]])),\n",
       "             ('model.7.attention_layer.W_V',\n",
       "              tensor([[[0.9531, 0.7445],\n",
       "                       [0.2115, 0.2110],\n",
       "                       [0.1407, 0.8599],\n",
       "                       [0.1538, 0.1409]],\n",
       "              \n",
       "                      [[0.5328, 0.1454],\n",
       "                       [0.2089, 0.3461],\n",
       "                       [0.1689, 0.3023],\n",
       "                       [0.5688, 0.6297]]])),\n",
       "             ('model.7.attention_layer.W_O',\n",
       "              tensor([[0.4735, 0.2407, 0.0175, 0.5039],\n",
       "                      [0.6432, 0.4393, 0.4009, 0.6190],\n",
       "                      [0.5670, 0.7510, 0.7163, 0.6936],\n",
       "                      [0.0668, 0.6660, 0.2326, 0.0024]])),\n",
       "             ('model.7.attention_layer.mask',\n",
       "              tensor([[0., -inf, -inf],\n",
       "                      [0., 0., -inf],\n",
       "                      [0., 0., 0.]])),\n",
       "             ('model.7.ffn.lin_1.weight',\n",
       "              tensor([[ 0.1677,  0.3958,  0.1221,  0.3785],\n",
       "                      [ 0.2786,  0.0410, -0.1374,  0.0904],\n",
       "                      [ 0.1461,  0.4504,  0.3269, -0.1948],\n",
       "                      [ 0.2419,  0.0520, -0.4613, -0.2888],\n",
       "                      [-0.2664,  0.4343,  0.3793,  0.2499],\n",
       "                      [-0.0950,  0.3529, -0.1066,  0.0070],\n",
       "                      [ 0.2431,  0.0743,  0.2334, -0.2243],\n",
       "                      [ 0.4937, -0.3827, -0.4223, -0.0730]])),\n",
       "             ('model.7.ffn.lin_1.bias',\n",
       "              tensor([-0.4446,  0.3904, -0.0008,  0.0934,  0.4108,  0.4239, -0.0687, -0.0953])),\n",
       "             ('model.7.ffn.lin_2.weight',\n",
       "              tensor([[ 0.1311,  0.0917, -0.1669, -0.3045,  0.1843, -0.0759, -0.1949,  0.3087],\n",
       "                      [-0.0986,  0.2591, -0.0345, -0.1814, -0.1806,  0.1024,  0.0657, -0.0536],\n",
       "                      [ 0.3263,  0.0033, -0.1366, -0.0319,  0.0818,  0.0853, -0.0637,  0.1262],\n",
       "                      [ 0.0096, -0.2215,  0.2465,  0.0221, -0.3133,  0.2167,  0.1214, -0.2132]])),\n",
       "             ('model.7.ffn.lin_2.bias',\n",
       "              tensor([-0.0764, -0.2811,  0.0060, -0.1561])),\n",
       "             ('model.7.norm.gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('model.7.norm.beta', tensor([0., 0., 0., 0.])),\n",
       "             ('model.8.attention_layer.W_Q',\n",
       "              tensor([[[0.3821, 0.4775],\n",
       "                       [0.8880, 0.3308],\n",
       "                       [0.2781, 0.2627],\n",
       "                       [0.9270, 0.4071]],\n",
       "              \n",
       "                      [[0.5125, 0.3954],\n",
       "                       [0.4109, 0.4092],\n",
       "                       [0.9295, 0.5813],\n",
       "                       [0.9634, 0.4360]]])),\n",
       "             ('model.8.attention_layer.W_K',\n",
       "              tensor([[[0.6437, 0.3584],\n",
       "                       [0.3677, 0.6305],\n",
       "                       [0.8753, 0.1739],\n",
       "                       [0.3073, 0.1081]],\n",
       "              \n",
       "                      [[0.4455, 0.5819],\n",
       "                       [0.1459, 0.9414],\n",
       "                       [0.6631, 0.9557],\n",
       "                       [0.1364, 0.0659]]])),\n",
       "             ('model.8.attention_layer.W_V',\n",
       "              tensor([[[0.1597, 0.2641],\n",
       "                       [0.0585, 0.0046],\n",
       "                       [0.0632, 0.6806],\n",
       "                       [0.9318, 0.1970]],\n",
       "              \n",
       "                      [[0.8690, 0.9062],\n",
       "                       [0.7071, 0.5337],\n",
       "                       [0.5455, 0.0347],\n",
       "                       [0.5613, 0.7378]]])),\n",
       "             ('model.8.attention_layer.W_O',\n",
       "              tensor([[0.7587, 0.3600, 0.3426, 0.6176],\n",
       "                      [0.6231, 0.5945, 0.1944, 0.0503],\n",
       "                      [0.4598, 0.0651, 0.1988, 0.2304],\n",
       "                      [0.9841, 0.4851, 0.9070, 0.2830]])),\n",
       "             ('model.8.attention_layer.mask',\n",
       "              tensor([[0., -inf, -inf],\n",
       "                      [0., 0., -inf],\n",
       "                      [0., 0., 0.]])),\n",
       "             ('model.8.ffn.lin_1.weight',\n",
       "              tensor([[ 3.6019e-01, -2.7044e-01,  4.4929e-01, -1.3519e-01],\n",
       "                      [-4.8654e-01, -9.1672e-02,  2.9616e-01,  1.6889e-01],\n",
       "                      [ 3.1316e-01, -1.0445e-01, -1.5187e-04, -5.1268e-02],\n",
       "                      [ 1.5560e-03,  3.3648e-01, -8.7987e-02, -4.9095e-01],\n",
       "                      [ 2.5897e-01, -3.8282e-02,  1.0914e-01, -1.7641e-01],\n",
       "                      [ 4.7093e-01, -6.7157e-02,  1.0487e-01, -4.6077e-01],\n",
       "                      [ 4.4467e-01,  1.2346e-02, -1.5620e-01,  1.0237e-01],\n",
       "                      [-2.0648e-01, -3.4307e-01, -1.6827e-01, -4.0307e-01]])),\n",
       "             ('model.8.ffn.lin_1.bias',\n",
       "              tensor([ 0.1485, -0.3232, -0.4938, -0.3651,  0.2195, -0.1679, -0.0509,  0.3199])),\n",
       "             ('model.8.ffn.lin_2.weight',\n",
       "              tensor([[-0.3301,  0.0650,  0.0056,  0.0143,  0.2110, -0.2780,  0.1435,  0.3422],\n",
       "                      [ 0.3146,  0.0460, -0.0192, -0.1471,  0.2681,  0.2669,  0.1531, -0.1151],\n",
       "                      [-0.2162, -0.3123, -0.1462,  0.1266, -0.1604, -0.0011, -0.0847, -0.1510],\n",
       "                      [ 0.3382,  0.1960,  0.2968,  0.2673,  0.0482, -0.0959, -0.0885, -0.3350]])),\n",
       "             ('model.8.ffn.lin_2.bias',\n",
       "              tensor([ 0.0434, -0.2923,  0.3118,  0.0789])),\n",
       "             ('model.8.norm.gamma', tensor([1., 1., 1., 1.])),\n",
       "             ('model.8.norm.beta', tensor([0., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(N=3, model_dim=4, key_dim=2, hidden_dim=8, num_heads=2, num_stack=9)\n",
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
