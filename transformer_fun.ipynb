{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cad8cd4-2333-4f29-8102-c854b13ed191",
   "metadata": {},
   "source": [
    "Transformer From Scratch \n",
    "========================\n",
    "\n",
    "Reading along Dan Jurafsky and James H. Martin's [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) book, I decided to follow through Chapter 8 of their book to implement a Transformer using Pytorch. It is my goal to have a working transformer which I can use to train on the guitar dataset. I know some linear algebra, the book essentially gives the entire algorithm in terms of linear algebra, and pytorch provides a nice but still very informative abstractions for doing linear algebra. I had no reason not to pursue this project on top of whatever I proposed to do initially. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5d5ef-478a-418e-b06a-6b4ab9361952",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70625c-5684-4d78-b22c-26775a11202e",
   "metadata": {},
   "source": [
    "Attention Layer\n",
    "---------------\n",
    "\n",
    "At the heart of Transformer is the **attention layer**. It is a mechanism that allows words(tokens) to gain contextual meaning from their surrounding words(tokens). It can have multiple **\"heads\"**, where each \"head\" can be thought of as a specialist who asks particular set of questions given some data. For instance, one head could focus solely on grammar while another could instead focus on sentiments (even though that might not be exactly what occurs under the hood).\n",
    "\n",
    "Each head's job, then, is to ask the right kinds of *questions* to choose which of previous words it has seen matters the most to the current word. To do this, each head consists of three main components: **Query**, **Key**, and **Value** weight matrices. \n",
    "\n",
    "<!-- \n",
    "    essentially, what it is at the end of the day is weighted sum, but it's obviously lot more complicated than that\n",
    "    don't forget to write out the equations that I have referenced\n",
    "    maybe throw in some pictures\n",
    "    say something about how masking and softmax is used to determine what key's to focus on\n",
    "    also explain how results from different heads are consolidated at the end\n",
    "--!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebd3896f-c9b2-4e50-b084-e0d5f232bfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 24])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from myTransformer import *\n",
    "batch_size = 10\n",
    "N = 10\n",
    "model_dim = 24\n",
    "num_heads = 4\n",
    "key_dim = 3\n",
    "\n",
    "M = 8\n",
    "X = torch.rand((batch_size, N, model_dim)) # batch_size is 10, 3 words represented as dim (1, 4) tensors\n",
    "Y = torch.rand((batch_size, M, model_dim)) # 3 words represented as dim (1, 4) tensors\n",
    "mask = torch.tensor([[0 if i>= j else -torch.inf for j in range(N)] for i in range(N)])\n",
    "\n",
    "multihead_attention = AttentionLayer(model_dim=model_dim, key_dim=key_dim, num_heads=num_heads)\n",
    "multihead_attention(X, H_enc=Y, mask=mask).shape\n",
    "#multihead_attention.to(\"cuda\")\n",
    "#multihead_attention(X.to(\"cuda\"), Y.to(\"cuda\"), mask=mask.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "243f5b74-0960-42aa-9b8b-44c15c21a603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.6556,  1.3237,  1.1348,  ...,  0.2511,  1.9615,  1.5982],\n",
       "         [ 1.9522,  2.2230,  0.7262,  ...,  1.6320,  2.2237,  0.9168],\n",
       "         [ 2.3365,  2.9451,  1.0538,  ...,  0.9670,  1.6536,  1.2317],\n",
       "         ...,\n",
       "         [ 3.0354,  1.3642,  0.6479,  ..., -0.0809,  1.7917,  2.5528],\n",
       "         [ 2.5161,  1.5389,  0.9710,  ..., -0.4273,  1.9307,  1.5794],\n",
       "         [ 3.3365,  2.4982,  0.4156,  ...,  0.1111,  1.9726,  1.2250]],\n",
       "\n",
       "        [[ 3.5571,  2.5597,  0.7519,  ...,  0.1864,  0.9485,  1.9511],\n",
       "         [ 2.9344,  2.3418,  0.7114,  ...,  0.4234,  1.7362,  1.8234],\n",
       "         [ 2.9155,  2.4235,  0.4709,  ..., -0.0989,  1.1049,  2.3805],\n",
       "         ...,\n",
       "         [ 3.3058,  2.1179,  0.2742,  ...,  0.9273,  1.4869,  1.9443],\n",
       "         [ 2.7348,  2.9225,  0.5708,  ...,  0.2487,  1.9907,  2.3717],\n",
       "         [ 2.4930,  2.1820,  1.4330,  ..., -0.2026,  1.7516,  2.4393]],\n",
       "\n",
       "        [[ 2.0348,  2.5245,  0.8779,  ...,  0.9297,  1.8288,  1.2047],\n",
       "         [ 1.4238,  2.3644,  0.9400,  ...,  1.2921,  1.6411,  1.8512],\n",
       "         [ 2.1868,  2.0185,  0.5255,  ...,  1.4676,  2.1160,  1.2504],\n",
       "         ...,\n",
       "         [ 3.4982,  1.9152,  0.6677,  ...,  0.9546,  2.1368,  1.2325],\n",
       "         [ 3.1125,  2.2948,  1.2821,  ...,  0.4925,  1.0342,  1.8204],\n",
       "         [ 2.6433,  2.2442,  1.5416,  ...,  1.1373,  1.8684,  1.0396]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.0528,  2.4270,  1.1510,  ...,  1.2447,  1.6435,  1.4438],\n",
       "         [ 3.4235,  2.6642,  0.4446,  ...,  1.5001,  1.4549,  0.9265],\n",
       "         [ 1.4988,  2.2609,  1.4282,  ...,  1.2804,  2.3700,  0.8211],\n",
       "         ...,\n",
       "         [ 2.2144,  3.0212,  1.1729,  ...,  0.9679,  1.5296,  0.5582],\n",
       "         [ 1.9051,  2.2603,  1.0192,  ...,  0.7856,  2.0560,  0.8080],\n",
       "         [ 2.1161,  1.9822,  0.5936,  ...,  0.8120,  1.2536,  1.4558]],\n",
       "\n",
       "        [[ 2.4926,  2.0670,  1.6894,  ...,  0.0980,  1.9137,  2.5948],\n",
       "         [ 2.3024,  2.4875,  1.1824,  ..., -0.5503,  1.3438,  2.9851],\n",
       "         [ 2.1296,  1.8981,  1.1253,  ..., -0.5136,  0.9780,  3.7278],\n",
       "         ...,\n",
       "         [ 3.6544,  1.7054,  0.0929,  ..., -0.1452,  1.2564,  2.9020],\n",
       "         [ 2.9788,  2.1185,  1.4981,  ...,  0.0758,  2.1365,  2.7199],\n",
       "         [ 2.8371,  2.2594,  0.8292,  ..., -0.0808,  0.9045,  3.6278]],\n",
       "\n",
       "        [[ 2.7306,  1.9126,  0.5423,  ...,  1.8345,  1.7334,  2.1002],\n",
       "         [ 2.6468,  2.0259,  0.6152,  ...,  0.5336,  1.6532,  1.8899],\n",
       "         [ 1.3074,  0.9868,  0.0047,  ...,  1.4626,  2.1069,  1.9604],\n",
       "         ...,\n",
       "         [ 1.8919,  1.9197,  0.6939,  ...,  0.9517,  2.0136,  1.8710],\n",
       "         [ 1.9122,  1.6824,  1.7333,  ...,  0.8030,  2.0753,  2.2976],\n",
       "         [ 1.7702,  2.1570,  0.8862,  ...,  0.8793,  2.3060,  1.7601]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack = TransformerStack(N=N, model_dim=model_dim, key_dim=key_dim, hidden_dim=8, num_heads=num_heads, num_stack=9)\n",
    "stack.state_dict()\n",
    "stack.train()\n",
    "stack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f98f9b15-b385-4993-8e49-a950a1af8a0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from https://pytorch-tutorials-preview.netlify.app/beginner/transformer_tutorial.html\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# i don't completely understand positional encoding yet, but I have built the intuition that \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# it is analogous to how binary numbers encode numbers; smaller bits flips more frequently \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# helps the model to figure out relative positioning...\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPositionalEncoding\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_dim: \u001b[38;5;28mint\u001b[39m, dropout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, max_len: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36mPositionalEncoding\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     pe[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(position \u001b[38;5;241m*\u001b[39m div_term)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpe\u001b[39m\u001b[38;5;124m'\u001b[39m, pe)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTensor\u001b[49m:\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m        x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe[:x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tensor' is not defined"
     ]
    }
   ],
   "source": [
    "# from https://pytorch-tutorials-preview.netlify.app/beginner/transformer_tutorial.html\n",
    "# i don't completely understand positional encoding yet, but I have built the intuition that \n",
    "# it is analogous to how binary numbers encode numbers; smaller bits flips more frequently \n",
    "# than larger bits; this is modeled by the sinusodial waves \n",
    "# it also takes advantage of linearity of trigonometric addition formulas, which supposedly \n",
    "# helps the model to figure out relative positioning...\n",
    "# https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2) * (-math.log(10000.0) / model_dim))\n",
    "        pe = torch.zeros(max_len, 1, model_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, X) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaffcf8-65af-4610-92b8-bb2234075775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what I need to do to finish up the encoder_decoder architecture \n",
    "# the only difference for the decoder architecture is the cross attention layer `\n",
    "# which is much like the self-attension layer except that it is using both the final \n",
    "# H of the encoder and that of decoder to do query-key matching, thus decoder needs to \n",
    "# take in memory from encoder "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
