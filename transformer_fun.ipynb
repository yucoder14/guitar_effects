{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1b780d83-22ec-4a07-8671-302956801997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class AttentionLayer(nn.Module): \n",
    "    def __init__(self,\n",
    "                 N,         # context length; how many tokens to consider at any point in time\n",
    "                 model_dim, # d\n",
    "                 key_dim,   # d_k\n",
    "                 num_heads=1, \n",
    "                ):\n",
    "        super().__init__() \n",
    "\n",
    "        if model_dim % num_heads != 0: \n",
    "            raise ValueError(\"model_dim is not divisible by num_heads!\")\n",
    "        \n",
    "        self.N = N\n",
    "        self.model_dim = model_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = model_dim//num_heads \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Query Weights (num_heads, model_dim, key_dim)\n",
    "        self.W_Q = nn.Parameter(torch.rand((num_heads, model_dim, key_dim)))\n",
    "        # Key Weights   (num_heads, model_dim, key_dim)\n",
    "        self.W_K = nn.Parameter(torch.rand((num_heads, model_dim, key_dim)))\n",
    "        # Value Weights (num_heads, model_dim, value_dim)\n",
    "        self.W_V = nn.Parameter(torch.rand((num_heads, model_dim, self.value_dim)))\n",
    "        # Output Weights\n",
    "        self.W_O = nn.Parameter(torch.rand((model_dim, model_dim)))\n",
    "\n",
    "        # Mask (for autoregressive model)\n",
    "        mask = torch.tensor([[0 if i>= j else -torch.inf for j in range(N)] for i in range(N)])\n",
    "        self.register_buffer(\"mask\", mask) # move mask to GPU\n",
    "        \n",
    "    def forward(self, X): # X has (N, d) dimensions\n",
    "        seq_len = X.shape[0]\n",
    "        Q = X@self.W_Q # (num_heads, N, key_dim)\n",
    "        K = X@self.W_K # (num_heads, N, key_dim)\n",
    "        V = X@self.W_V # (num_heads, N, value_dim)\n",
    "\n",
    "        current_mask = self.mask[:seq_len, :seq_len] # when seq_len < N \n",
    "\n",
    "        attention = Q@(K.mT) / math.sqrt(key_dim) + current_mask # (num_heads, N (queries), N (keys)) \n",
    "        heads = nn.functional.softmax(attention, dim = -1)@V #(num_heads, N, value_dim)\n",
    "        cat_heads = torch.cat(heads.unbind(), dim=1) # (N, value_dim) each and concatenate the columns to form (N, model_dim)\n",
    "        A = cat_heads@self.W_O \n",
    "\n",
    "        return A\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "253af888-f9bf-485b-bada-bd37ec4d1870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7208, 0.9492],\n",
      "         [1.3220, 1.5685],\n",
      "         [1.1685, 1.4063]],\n",
      "\n",
      "        [[0.6066, 0.5019],\n",
      "         [1.2620, 1.2463],\n",
      "         [1.1876, 1.0844]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[1.5921, 0.5447, 1.7739, 1.8473],\n",
      "        [3.0285, 1.0847, 3.2703, 3.5603],\n",
      "        [2.7400, 0.9735, 2.9330, 3.1974]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_dim = 4\n",
    "key_dim = 2\n",
    "N = 3\n",
    "num_heads = 2\n",
    "value_dim = model_dim // num_heads\n",
    "X = torch.rand((N,model_dim))\n",
    "W_Q = torch.rand((num_heads, model_dim, key_dim), requires_grad=True)\n",
    "W_K = torch.rand((num_heads, model_dim, key_dim), requires_grad=True)\n",
    "W_V = torch.rand((num_heads, model_dim, value_dim), requires_grad=True)\n",
    "W_O = torch.rand((model_dim, model_dim), requires_grad=True)\n",
    "mask = torch.tensor([[0 if i>= j else -torch.inf for j in range(N)] for i in range(N)])\n",
    "\n",
    "Q = X@W_Q # (num_heads, N, key_dim)\n",
    "K = X@W_K # (num_heads, N, key_dim)\n",
    "V = X@W_V # (num_heads, N, value_dim)\n",
    "attention = Q@(K.mT) / math.sqrt(key_dim) + mask # (num_heads, N (queries), N (keys)) \n",
    "heads = nn.functional.softmax(attention, dim = -1)@V #(num_heads, N, value_dim)\n",
    "cat_heads = torch.cat(heads.unbind(), dim=1)\n",
    "A = cat_heads@W_O\n",
    "print(heads)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "be67425b-4a19-431d-be3a-0cf9966aacae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3525, 0.8059, 1.0190, 1.8855],\n",
       "        [2.5131, 1.6030, 1.9719, 3.5664],\n",
       "        [2.3800, 1.5152, 1.8651, 3.3762]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attention = AttentionLayer(N=3, model_dim=4, key_dim=2, num_heads=2)\n",
    "multihead_attention(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
