{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cad8cd4-2333-4f29-8102-c854b13ed191",
   "metadata": {},
   "source": [
    "Transformer From Scratch \n",
    "========================\n",
    "\n",
    "Reading along Dan Jurafsky and James H. Martin's [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) book, I decided to follow through Chapter 8 of their book to implement a Transformer using Pytorch. It is my goal to have a working transformer which I can use to train on the guitar dataset. I know some linear algebra, the book essentially gives the entire algorithm in terms of linear algebra, and pytorch provides a nice but still very informative abstractions for doing linear algebra. I had no reason not to pursue this project on top of whatever I proposed to do initially. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5d5ef-478a-418e-b06a-6b4ab9361952",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70625c-5684-4d78-b22c-26775a11202e",
   "metadata": {},
   "source": [
    "Attention Layer\n",
    "---------------\n",
    "\n",
    "At the heart of Transformer is the **attention layer**. It is a mechanism that allows words(tokens) to gain contextual meaning from their surrounding words(tokens). It can have multiple **\"heads\"**, where each \"head\" can be thought of as a specialist who asks particular set of questions given some data. For instance, one head could focus solely on grammar while another could instead focus on sentiments (even though that might not be exactly what occurs under the hood).\n",
    "\n",
    "Each head's job, then, is to ask the right kinds of *questions* to choose which of previous words it has seen matters the most to the current word. To do this, each head consists of three main components: **Query**, **Key**, and **Value** weight matrices. \n",
    "\n",
    "<!-- \n",
    "    essentially, what it is at the end of the day is weighted sum, but it's obviously lot more complicated than that\n",
    "    don't forget to write out the equations that I have referenced\n",
    "    maybe throw in some pictures\n",
    "    say something about how masking and softmax is used to determine what key's to focus on\n",
    "    also explain how results from different heads are consolidated at the end\n",
    "--!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b780d83-22ec-4a07-8671-302956801997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class AttentionLayer(nn.Module): \n",
    "    def __init__(self,\n",
    "                 model_dim, # d\n",
    "                 key_dim,   # d_k\n",
    "                 num_heads=1\n",
    "                ):\n",
    "        super().__init__() \n",
    "\n",
    "        if num_heads < 1:\n",
    "            raise ValueError(\"num_heads cannot be less than 1!\")\n",
    "\n",
    "        if model_dim % num_heads != 0: \n",
    "            raise ValueError(\"model_dim is not divisible by num_heads!\")\n",
    "        \n",
    "        self.model_dim = model_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = model_dim//num_heads \n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # without batching considerations...\n",
    "        #self.W_Q = nn.Parameter(torch.rand((num_heads, model_dim, key_dim))) \n",
    "        #self.W_K = nn.Parameter(torch.rand((num_heads, model_dim, key_dim))) \n",
    "        #self.W_V = nn.Parameter(torch.rand((num_heads, model_dim, self.value_dim))) \n",
    "        # self.W_O = nn.Parameter(torch.rand((model_dim, model_dim)))\n",
    "\n",
    "        # using nn.Linear to automatically do batching  \n",
    "        # concatenate heads vertically for broadcasting\n",
    "        self.W_Q = nn.Linear(in_features=model_dim, out_features=num_heads * key_dim, bias=False)\n",
    "        self.W_K = nn.Linear(in_features=model_dim, out_features=num_heads * key_dim, bias=False)\n",
    "        self.W_V = nn.Linear(in_features=model_dim, out_features=num_heads * self.value_dim, bias=False)\n",
    "        self.W_O = nn.Linear(in_features=num_heads * self.value_dim, out_features=model_dim, bias=False)\n",
    "        \n",
    "    def forward(self, X, H_enc=None, mask=None): # X has (batch_size, N, model_dim) dimensions\n",
    "        K_input = H_enc if H_enc is not None else X\n",
    "        V_input = H_enc if H_enc is not None else X\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        N = X.shape[1]\n",
    "        M = K_input.shape[1]\n",
    "\n",
    "        # without batching considerations \n",
    "        #Q = X@self.W_Q # (num_heads, N, key_dim)\n",
    "        #K = K_input@self.W_K # (num_heads, N, key_dim)\n",
    "        #V = V_input@self.W_V # (num_heads, N, value_dim)\n",
    "\n",
    "        Q = self.W_Q(X)       # (batch_size, N, num_heads * key_dim)   \n",
    "        K = self.W_K(K_input) # (batch_size, M, num_heads * key_dim) \n",
    "        V = self.W_V(V_input) # (batch_size, M, num_heads * key_dim) \n",
    "\n",
    "        Q = Q.view(batch_size, N, self.num_heads, self.key_dim).transpose(1,2) \n",
    "        K = K.view(batch_size, M, self.num_heads, self.key_dim).transpose(1,2)\n",
    "        V = V.view(batch_size, M, self.num_heads, self.value_dim).transpose(1,2)\n",
    "\n",
    "        attention = Q@(K.mT) / math.sqrt(self.key_dim) # (batch_size, num_heads, N (queries), M (keys)) \n",
    "        if mask is not None:\n",
    "            current_mask = mask[:N, :M] # I should consider lengths of N, M carefully \n",
    "            attention += current_mask \n",
    "            \n",
    "        prob = nn.functional.softmax(attention, dim = -1) # dim -1 should be keys \n",
    "        values = prob@V #(batch_size, num_heads, N, value_dim)\n",
    "        values_cat = values.transpose(1, 2).contiguous().view(batch_size, N, -1) # (batch_size, N, num_heads * value_dim)\n",
    "        \n",
    "        #cat_heads = torch.cat(heads.unbind(), dim=1) # (N, value_dim) each and concatenate the columns to form (N, model_dim)\n",
    "        A = self.W_O(values_cat) # (N, model_dim)\n",
    "\n",
    "        return A\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd3896f-c9b2-4e50-b084-e0d5f232bfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 24])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "N = 10\n",
    "model_dim = 24\n",
    "num_heads = 4\n",
    "key_dim = 3\n",
    "\n",
    "M = 8\n",
    "X = torch.rand((batch_size, N, model_dim)) # batch_size is 10, 3 words represented as dim (1, 4) tensors\n",
    "Y = torch.rand((batch_size, M, model_dim)) # 3 words represented as dim (1, 4) tensors\n",
    "mask = torch.tensor([[0 if i>= j else -torch.inf for j in range(N)] for i in range(N)])\n",
    "\n",
    "multihead_attention = AttentionLayer(model_dim=model_dim, key_dim=key_dim, num_heads=num_heads)\n",
    "multihead_attention(X, H_enc=Y, mask=mask).shape\n",
    "#multihead_attention.to(\"cuda\")\n",
    "#multihead_attention(X.to(\"cuda\"), Y.to(\"cuda\"), mask=mask.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e46ee8d-91a1-42b7-8d35-60d37716c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(in_features=model_dim, out_features=hidden_dim)\n",
    "        self.lin_2 = nn.Linear(in_features=hidden_dim, out_features=model_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, X):\n",
    "        return self.lin_2(self.relu(self.lin_1(X)))\n",
    "\n",
    "class LayerNorm(nn.Module): \n",
    "    def __init__(self, model_dim, epsilon=1e-6): \n",
    "        super().__init__()\n",
    "        # gamma and beta is to normalize features \n",
    "        self.gamma = nn.Parameter(torch.ones(model_dim)) # initialize gammas to ones because if initialized randomly to 0, it's dead signal\n",
    "        self.beta = nn.Parameter(torch.zeros(model_dim))\n",
    "        self.eps = epsilon\n",
    "        \n",
    "    def forward(self, X): # (batch_size, N, model_dim)\n",
    "        model_dim = X.shape[-1]\n",
    "        mean = torch.mean(X, dim=-1, keepdims=True)\n",
    "        std = torch.std(X, dim=-1, keepdims=True)\n",
    "        X_hat = (X - mean) / (std + self.eps) # add epsilon for numerical stability\n",
    "        layer_norm = self.gamma * X_hat + self.beta\n",
    "        return layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f47317f-3adc-4600-8013-7aad81f09f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24]) torch.Size([24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[100.9493, 100.2801, 100.1258,  ..., 100.2354, 100.9412, 100.4811],\n",
       "         [100.0720, 100.8310, 100.8976,  ..., 100.8133, 100.0188, 100.8997],\n",
       "         [100.1473, 100.7575, 100.9484,  ..., 100.8251, 100.1841, 100.3081],\n",
       "         ...,\n",
       "         [100.2794, 100.0046, 100.7232,  ..., 100.8640, 100.9785, 100.0878],\n",
       "         [100.8674, 100.9698, 100.3904,  ..., 100.3108, 100.5854, 100.9261],\n",
       "         [100.2624, 100.3437, 100.0919,  ..., 100.0314, 100.0253, 100.1013]],\n",
       "\n",
       "        [[100.8234, 100.7289, 100.7664,  ..., 100.5530, 100.8430, 100.4411],\n",
       "         [100.7440, 100.0608, 100.2004,  ..., 100.1383, 100.9858, 100.4086],\n",
       "         [100.1379, 100.4247, 100.3906,  ..., 100.4993, 100.9951, 100.6967],\n",
       "         ...,\n",
       "         [100.0161, 100.8644, 100.0283,  ..., 100.9211, 100.3531, 100.8766],\n",
       "         [100.1065, 100.1919, 100.6886,  ..., 100.6221, 100.0700, 100.5314],\n",
       "         [100.0892, 100.5095, 100.7930,  ..., 100.8587, 100.5753, 100.9021]],\n",
       "\n",
       "        [[100.8972, 100.8424, 100.2850,  ..., 100.6089, 100.8277, 100.3910],\n",
       "         [100.2558, 100.1787, 100.4840,  ..., 100.7825, 100.2475, 100.0244],\n",
       "         [100.9921, 100.0199, 100.7737,  ..., 100.7848, 100.3100, 100.5280],\n",
       "         ...,\n",
       "         [100.5950, 100.0037, 100.9896,  ..., 100.2985, 100.9667, 100.4564],\n",
       "         [100.0616, 100.2911, 100.7759,  ..., 100.3992, 100.4513, 100.1278],\n",
       "         [100.2296, 100.4336, 100.9113,  ..., 100.1489, 100.0802, 100.9229]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[100.7683, 100.6092, 100.2774,  ..., 100.0505, 100.3400, 100.4227],\n",
       "         [100.1627, 100.8156, 100.3563,  ..., 100.8399, 100.4616, 100.5605],\n",
       "         [100.1891, 100.2124, 100.4090,  ..., 100.3952, 100.7307, 100.3018],\n",
       "         ...,\n",
       "         [100.7954, 100.9445, 100.8375,  ..., 100.9801, 100.9244, 100.1697],\n",
       "         [100.5151, 100.9412, 100.1111,  ..., 100.2964, 100.0188, 100.3271],\n",
       "         [100.1943, 100.1253, 100.1605,  ..., 100.0525, 100.7194, 100.6144]],\n",
       "\n",
       "        [[100.0273, 100.9619, 100.7353,  ..., 100.5441, 100.2818, 100.6047],\n",
       "         [100.6296, 100.1638, 100.0585,  ..., 100.3860, 100.1364, 100.2894],\n",
       "         [100.6059, 100.1644, 100.1132,  ..., 100.5955, 100.4491, 100.8241],\n",
       "         ...,\n",
       "         [100.9735, 100.8137, 100.7420,  ..., 100.8984, 100.4541, 100.1996],\n",
       "         [100.2647, 100.2265, 100.3719,  ..., 100.6561, 100.2145, 100.8301],\n",
       "         [100.0907, 100.0239, 100.8513,  ..., 100.1064, 100.6581, 100.8052]],\n",
       "\n",
       "        [[100.9988, 100.6169, 100.0241,  ..., 100.7888, 100.0533, 100.0007],\n",
       "         [100.5850, 100.2469, 100.2660,  ..., 100.4139, 100.5884, 100.0941],\n",
       "         [100.1669, 100.9524, 100.3642,  ..., 100.3129, 100.8145, 100.5358],\n",
       "         ...,\n",
       "         [100.8497, 100.5282, 100.5366,  ..., 100.4393, 100.6113, 100.7457],\n",
       "         [100.8770, 100.3176, 100.5589,  ..., 100.7845, 100.9745, 100.0050],\n",
       "         [100.4305, 100.1095, 100.5603,  ..., 100.1626, 100.2380, 100.8679]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = FeedForward(model_dim=model_dim, hidden_dim=12)\n",
    "ff(X)\n",
    "norm = LayerNorm(model_dim=model_dim)\n",
    "norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61771565-f4a4-404b-acfa-c9effe36067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module): \n",
    "    def __init__(self, \n",
    "                 N, \n",
    "                 model_dim, \n",
    "                 key_dim, \n",
    "                 hidden_dim, \n",
    "                 num_heads=1,\n",
    "                 cross_attention=False\n",
    "                ): \n",
    "        super().__init__() \n",
    "        self.N = N\n",
    "        self.model_dim = model_dim\n",
    "        self.key_dim = key_dim \n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.num_heads = num_heads\n",
    "        self.cross_attention = cross_attention\n",
    "        \n",
    "        self.attention_layer = AttentionLayer(model_dim=model_dim, key_dim=key_dim, num_heads=num_heads)\n",
    "        self.ffn = FeedForward(model_dim=model_dim, hidden_dim=hidden_dim)\n",
    "        self.norm_1 = LayerNorm(model_dim=model_dim)\n",
    "        self.norm_2 = LayerNorm(model_dim=model_dim)\n",
    "\n",
    "        if self.cross_attention:\n",
    "            self.cross_attention_layer = AttentionLayer(model_dim=model_dim, key_dim=key_dim, num_heads=num_heads)\n",
    "            self.norm_3 = LayerNorm(model_dim=model_dim)\n",
    "\n",
    "    def forward(self, X, H_enc=None, mask=None): \n",
    "        T_1 = self.norm_1(X) \n",
    "        T_2 = self.attention_layer(T_1, mask=mask)\n",
    "        T_3 = T_2 + X\n",
    "        if self.cross_attention: \n",
    "            if H_enc is None:\n",
    "                raise ValueError(\"H_enc cannot be None if cross_attention is enabled!\")\n",
    "            T_a = self.norm_3(T_3) \n",
    "            T_b = self.cross_attention_layer(T_a, H_enc=H_enc)\n",
    "            T_3 += T_b\n",
    "        T_4 = self.norm_2(T_3)\n",
    "        T_5 = self.ffn(T_4)\n",
    "        H = T_5 + T_3 \n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "218580c8-ed94-466a-b480-1638781b64aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5950,  0.6532, -0.0883,  ...,  0.8696,  0.9251,  0.5034],\n",
       "         [ 0.1859,  0.9912,  0.9921,  ...,  1.2090, -0.1045,  0.7301],\n",
       "         [ 0.1552,  1.0726,  0.9090,  ...,  1.3952,  0.0828,  0.1536],\n",
       "         ...,\n",
       "         [ 0.1655,  0.2254,  0.4131,  ...,  1.6077,  1.0697, -0.1074],\n",
       "         [ 0.5230,  1.3601,  0.4634,  ...,  0.6692,  0.5866,  1.0162],\n",
       "         [ 0.2869,  0.6113,  0.0617,  ...,  0.6822, -0.1595, -0.0758]],\n",
       "\n",
       "        [[ 0.3425,  1.2843,  0.5048,  ...,  0.9217,  1.1042,  0.7714],\n",
       "         [ 0.8797,  0.4325,  0.0953,  ...,  0.7544,  1.1505,  0.9455],\n",
       "         [ 0.1512,  0.3505,  0.6392,  ...,  0.5101,  1.2656,  0.6963],\n",
       "         ...,\n",
       "         [ 0.3292,  0.8944,  0.0065,  ...,  1.2100,  0.6854,  0.7299],\n",
       "         [ 0.4246,  0.1905,  0.8780,  ...,  0.6986,  0.3605,  0.4341],\n",
       "         [ 0.0440,  0.5063,  0.9983,  ...,  0.7885,  0.7122,  0.9352]],\n",
       "\n",
       "        [[ 0.9221,  0.7674,  0.2664,  ...,  1.2873,  0.8794,  0.8752],\n",
       "         [-0.0435, -0.1394,  0.7310,  ...,  0.8797,  0.2774,  0.0813],\n",
       "         [ 1.3007, -0.1638,  0.5163,  ...,  1.6041,  0.6633,  0.4894],\n",
       "         ...,\n",
       "         [ 0.4916, -0.1033,  1.1200,  ...,  0.4564,  0.8207,  0.3836],\n",
       "         [ 0.1104,  0.1549,  0.5313,  ...,  1.1346,  0.5933,  0.0166],\n",
       "         [ 0.4158,  0.2742,  0.9042,  ...,  0.7544,  0.1268,  0.8666]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5266,  0.7369,  0.0986,  ...,  0.8970,  0.0892,  0.3048],\n",
       "         [ 0.0994,  0.8907, -0.0081,  ...,  1.8150,  0.2347,  0.2913],\n",
       "         [ 0.2815,  0.1464,  0.2896,  ...,  1.2363,  0.5909,  0.4238],\n",
       "         ...,\n",
       "         [ 0.3737,  0.8831,  0.7280,  ...,  1.8951,  1.0773,  0.0518],\n",
       "         [ 0.4798,  0.9108, -0.1774,  ...,  1.3058, -0.0710,  0.0882],\n",
       "         [ 0.1028,  0.0478,  0.1325,  ...,  0.7642,  0.6115,  0.6651]],\n",
       "\n",
       "        [[ 0.0499,  0.8101,  0.6876,  ...,  0.9937,  0.3920,  0.4640],\n",
       "         [ 0.7694,  0.0677, -0.0227,  ...,  1.1503,  0.4026,  0.5621],\n",
       "         [ 0.5156, -0.0079,  0.1603,  ...,  1.0961,  0.6199,  0.8124],\n",
       "         ...,\n",
       "         [ 0.8578,  0.8056,  0.3560,  ...,  1.6394,  0.6917,  0.1122],\n",
       "         [ 0.0931, -0.2497,  0.7048,  ...,  0.7219,  0.6356,  0.6970],\n",
       "         [-0.0384,  0.0417,  0.4164,  ...,  0.7896,  0.9567,  0.5978]],\n",
       "\n",
       "        [[ 0.5325,  0.8178, -0.1978,  ...,  1.4413,  0.4140, -0.0568],\n",
       "         [ 0.7094,  0.3772,  0.1485,  ...,  1.0678,  0.6778,  0.0546],\n",
       "         [ 0.0460,  1.2611,  0.1421,  ...,  0.9245,  0.8661,  0.4355],\n",
       "         ...,\n",
       "         [ 0.8687,  0.3286,  0.4945,  ...,  0.9073,  1.1961,  0.7981],\n",
       "         [ 0.8895,  0.3953,  0.4820,  ...,  1.2859,  0.9795, -0.0642],\n",
       "         [ 0.4059,  0.4544,  0.2252,  ...,  0.6949,  0.1108,  0.7882]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_block = TransformerBlock(N=N, model_dim=model_dim, key_dim=key_dim, hidden_dim=8, num_heads=num_heads)\n",
    "decoder_block = TransformerBlock(N=N, model_dim=model_dim, key_dim=key_dim, hidden_dim=8, num_heads=num_heads, cross_attention=True)\n",
    "encoder_block(X,mask=mask)\n",
    "decoder_block(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "380b1c0f-ebb3-4274-abef-d6eba5741559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerStack(nn.Module): \n",
    "    def __init__(self, \n",
    "                 N, \n",
    "                 model_dim, \n",
    "                 key_dim, \n",
    "                 hidden_dim, \n",
    "                 num_heads=1,\n",
    "                 cross_attention=False,\n",
    "                 num_stack=1\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_stack < 1: \n",
    "            raise ValueError(\"num_stack cannot be less than 1!\")\n",
    "\n",
    "        # using module list a\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(N, model_dim, key_dim, hidden_dim, num_heads, cross_attention=cross_attention) \n",
    "             for _ in range(num_stack)\n",
    "        ])\n",
    "    def forward(self, X, H_enc=None, mask=None): \n",
    "        for block in self.blocks:\n",
    "            X = block(X, H_enc, mask)\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "243f5b74-0960-42aa-9b8b-44c15c21a603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6120,  0.5040,  0.3937,  ...,  2.0055, -1.2134,  0.9813],\n",
       "         [ 0.7487,  0.2654,  0.1714,  ...,  1.7507, -1.4817,  2.1662],\n",
       "         [ 0.6117,  0.7551,  0.4294,  ...,  2.4666, -2.3493,  0.7458],\n",
       "         ...,\n",
       "         [ 0.6705,  0.2939,  0.7746,  ...,  2.3897, -1.4351,  1.0199],\n",
       "         [ 1.4329,  0.9844,  0.4120,  ...,  1.9334, -1.7569,  1.4429],\n",
       "         [ 1.1453,  0.5180,  0.2179,  ...,  1.2895, -2.1818,  0.3902]],\n",
       "\n",
       "        [[ 0.9372,  0.2691,  0.6632,  ...,  1.8215, -1.2884, -0.1815],\n",
       "         [ 0.6051,  0.4189,  0.4812,  ...,  1.6336, -1.1065, -0.1776],\n",
       "         [ 0.1922,  0.2067,  0.5248,  ...,  1.2971, -0.9452,  1.3682],\n",
       "         ...,\n",
       "         [-0.3864,  1.0897,  0.0713,  ...,  1.4096, -1.7086,  1.2176],\n",
       "         [-0.0498,  0.1457,  0.7907,  ...,  1.7237, -1.9853,  0.7064],\n",
       "         [ 0.1061,  0.6879,  0.8388,  ...,  1.4432, -1.7721,  1.8719]],\n",
       "\n",
       "        [[ 0.4825,  0.7374,  0.0675,  ...,  2.1558, -0.2209, -0.0694],\n",
       "         [-0.3750,  0.3154,  0.1831,  ...,  2.5020, -1.6571, -0.5285],\n",
       "         [ 0.4410,  0.4187,  0.7399,  ...,  2.5990, -1.2187, -0.4074],\n",
       "         ...,\n",
       "         [ 0.2316, -0.1045,  0.6073,  ...,  2.4025, -0.3801, -0.2887],\n",
       "         [-0.3581,  0.4029,  0.4864,  ...,  2.1328, -0.9067, -0.3199],\n",
       "         [ 0.0046,  0.3983,  0.2145,  ...,  1.8831, -1.7519, -0.1127]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5531,  1.3968, -1.0753,  ...,  0.4082, -2.3690, -0.1040],\n",
       "         [-0.4934,  1.5246, -0.8364,  ...,  1.3815, -2.0063,  0.1465],\n",
       "         [-0.1699,  1.0161, -0.5929,  ...,  1.5739, -1.1042, -0.0586],\n",
       "         ...,\n",
       "         [ 0.7046,  2.1792,  0.1383,  ...,  2.3172, -1.8154, -0.7255],\n",
       "         [ 0.3897,  1.7872, -0.8623,  ...,  0.6837, -1.9997, -0.6394],\n",
       "         [-0.2335,  0.9141, -1.0275,  ...,  1.0232, -2.1591,  0.4916]],\n",
       "\n",
       "        [[ 0.0903,  0.2967,  0.6384,  ...,  1.4450, -1.5152,  1.2736],\n",
       "         [ 0.9545, -0.3003,  0.0223,  ...,  1.8735, -1.6213, -0.1418],\n",
       "         [ 0.4015, -0.2171,  0.3885,  ...,  1.7809, -1.3213,  1.1282],\n",
       "         ...,\n",
       "         [ 1.1948,  0.3268,  0.6886,  ...,  2.4558, -1.0806, -0.2124],\n",
       "         [ 0.1941, -0.3996,  0.6759,  ...,  1.7674, -1.5154,  1.4293],\n",
       "         [ 0.1329, -0.6128,  1.1954,  ...,  1.5342, -1.3884,  1.1565]],\n",
       "\n",
       "        [[ 1.1599,  0.7364, -0.1486,  ...,  1.4913, -2.4709, -0.0521],\n",
       "         [ 0.6473,  0.8938,  0.4301,  ...,  1.3523, -2.4974,  0.4322],\n",
       "         [ 0.4321,  0.8652,  0.2119,  ...,  1.3794, -1.8323,  1.1654],\n",
       "         ...,\n",
       "         [ 1.3019,  0.3474,  0.7277,  ...,  1.5507, -2.3004,  1.8316],\n",
       "         [ 1.4117,  0.4943,  0.5831,  ...,  1.7246, -2.2737,  0.3380],\n",
       "         [ 1.4464, -0.5324, -0.1679,  ...,  0.6398, -2.6867,  1.9054]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerStack(N=N, model_dim=model_dim, key_dim=key_dim, hidden_dim=8, num_heads=num_heads, num_stack=9)\n",
    "model.state_dict()\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f9b15-b385-4993-8e49-a950a1af8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch-tutorials-preview.netlify.app/beginner/transformer_tutorial.html\n",
    "# i don't completely understand positional encoding yet, but I have built the intuition that \n",
    "# it is analogous to how binary numbers encode numbers; smaller bits flips more frequently \n",
    "# than larger bits; this is modeled by the sinusodial waves \n",
    "# it also takes advantage of linearity of trigonometric addition formulas, which supposedly \n",
    "# helps the model to figure out relative positioning...\n",
    "# https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2) * (-math.log(10000.0) / model_dim))\n",
    "        pe = torch.zeros(max_len, 1, model_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaffcf8-65af-4610-92b8-bb2234075775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what I need to do to finish up the encoder_decoder architecture \n",
    "# the only difference for the decoder architecture is the cross attention layer `\n",
    "# which is much like the self-attension layer except that it is using both the final \n",
    "# H of the encoder and that of decoder to do query-key matching, thus decoder needs to \n",
    "# take in memory from encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c93ee1-7055-47af-a631-ddbaab62cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "\n",
    "# not the most efficient way to do so, but in the way that my mind thinks about things\n",
    "N = 5\n",
    "model_dim = 4\n",
    "batch_size = 10\n",
    "key_dim = 2\n",
    "num_heads = 2\n",
    "num_batch = 10\n",
    "value_dim = model_dim // num_heads\n",
    "mask = torch.tensor([[0 if i>= j else -torch.inf for j in range(N)] for i in range(N)])\n",
    "\n",
    "X = torch.ones((num_batch, N, model_dim))\n",
    "X[:,1] = 2\n",
    "X[:,:,2] = 3\n",
    "print(X) # should not be symmetrical for demonstration purposes\n",
    "single_head = torch.ones((model_dim, key_dim))\n",
    "single_head_v = torch.ones((model_dim, value_dim))\n",
    "W_Q = torch.cat([single_head * i for i in range(2, 2 + num_heads)], dim=1)\n",
    "W_K = torch.cat([single_head * i for i in range(4, 4 + num_heads)], dim=1)\n",
    "W_V = torch.cat([single_head_v * i for i in range(6, 6 + num_heads)], dim=1)\n",
    "W_O = torch.rand((model_dim, model_dim))\n",
    "print(X.shape)\n",
    "print(W_Q.shape)\n",
    "Q = X@W_Q\n",
    "K = X@W_K\n",
    "V = X@W_V\n",
    "print(Q.shape) # batch_size, N, key_dim * num_heads\n",
    "print(Q.unbind()[0]) # essentially Q's from different heads concatenated to be next to each other \"vertically\"\n",
    "Q_reshaped = Q.view(num_batch, N, num_heads, key_dim).transpose(1,2) \n",
    "K_reshaped = K.view(num_batch, N, num_heads, key_dim).transpose(1,2)\n",
    "V_reshaped = V.view(num_batch, N, num_heads, value_dim).transpose(1,2)\n",
    "print(Q_reshaped.unbind()[0])\n",
    "print(Q.data_ptr() == Q_reshaped.data_ptr()) # this should be False, meaning reshape has created new tensor, which is not memory efficient\n",
    "attention = Q_reshaped@K_reshaped.mT / math.sqrt(key_dim) + mask # mask broadcasting\n",
    "print(attention.shape, V_reshaped.shape)\n",
    "probs = torch.nn.functional.softmax(attention, dim=-1) \n",
    "values = probs@V_reshaped / 24 # just dividing by arbitrary number for ease of seeing value matrix of each head as a unified number\n",
    "print(values.shape) # batch_size, num_heads, N, value_dim\n",
    "# batch_size, num_heads, N, value_dim --> batch_size, num_heads, value_dim, N --> batch_size, num_heads*value_dim, N --> batch_size, N, num_heads*value_dim (model_dim)\n",
    "values_cat = values.transpose(-2, -1).flatten(start_dim=1, end_dim=2).transpose(-2, -1)\n",
    "# batch_size, num_heads, value_dim, N --> batch_size, N, num_heads, value_dim --> batch_size, N, num_heads*value_dim\n",
    "values_cat_2 = values.transpose(1, 2).contiguous().view(batch_size, N, -1)\n",
    "values_cat == values_cat_2\n",
    "#values_cat@W_O"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
